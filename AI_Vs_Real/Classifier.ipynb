{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nQNUObJ_FQKE",
        "outputId": "6dec5d60-06d7-4cb0-bdab-a2e92ae9c06d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_preprocessing"
      ],
      "metadata": {
        "id": "9Efnj4FbIexQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras_preprocessing.image import load_img\n",
        "from keras.models import Sequential\n",
        "from keras.applications import MobileNetV2, ResNet152, VGG16, EfficientNetB0, InceptionV3\n",
        "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers\n",
        "import tensorflow as tf\n",
        "\n",
        "temp_dir = []\n",
        "sqlen = 289\n",
        "\n",
        "def createdataframe(dir):\n",
        "    image_paths = []\n",
        "\n",
        "    labels = []\n",
        "    for label in os.listdir(dir):\n",
        "        for imagename in os.listdir(os.path.join(dir, label)):\n",
        "            image_paths.append(os.path.join(dir, label, imagename))\n",
        "            labels.append(label)\n",
        "        print(label, \"completed\")\n",
        "    return image_paths, labels\n",
        "\n",
        "def extractrain_features(images):\n",
        "    features = []\n",
        "\n",
        "    for image in tqdm(images):\n",
        "        img = load_img(image, target_size=(sqlen, sqlen))\n",
        "        img = np.array(img)\n",
        "        features.append(img)\n",
        "\n",
        "    features = np.array(features)\n",
        "    features = features.reshape(features.shape[0], sqlen,sqlen , 3)  # Reshape all images in one go\n",
        "    return features\n",
        "\n",
        "def extractest_features(images):\n",
        "    features = []\n",
        "    for image in tqdm(images):\n",
        "      try:\n",
        "        img = load_img(image, target_size=(sqlen, sqlen))\n",
        "        img = np.array(img)\n",
        "        features.append(img)\n",
        "\n",
        "      except Exception as e:\n",
        "        print(image)\n",
        "        temp_dir.append(image)\n",
        "        continue\n",
        "\n",
        "    for i in (temp_dir):\n",
        "       test_paths.remove(i)\n",
        "\n",
        "    features = np.array(features)\n",
        "    features = features.reshape(features.shape[0], sqlen, sqlen, 3)  # Reshape all images in one go\n",
        "    return features\n",
        "\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/New_Data/Train\"\n",
        "\n",
        "train = pd.DataFrame()\n",
        "train['image'], train['label'] = createdataframe(TRAIN_DIR)\n",
        "\n",
        "train_features = extractrain_features(train['image'])\n",
        "\n",
        "x_train = train_features / 255.0\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(train['label'])\n",
        "y_train = le.transform(train['label'])\n",
        "y_train = to_categorical(y_train, num_classes=2)\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=(sqlen, sqlen, 3), padding='same'))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "\n",
        "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(1024, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.Huber(delta=1.0), metrics=['mae'])\n",
        "# Fit model\n",
        "model.fit(x=x_train,y=y_train, batch_size=20, epochs=20)\n",
        "\n",
        "TEST_DIR = \"/content/drive/MyDrive/induction-task-2025/Test_Images\"\n",
        "test_images = sorted([img for img in os.listdir(TEST_DIR)],\n",
        "                     key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
        "test_paths = [os.path.join(TEST_DIR, img) for img in test_images]\n",
        "\n",
        "x_test = extractest_features(test_paths)\n",
        "x_test = x_test / 255.0  # Normalize test data\n",
        "\n",
        "\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "predicted_labels = le.inverse_transform(np.argmax(predictions, axis=1))\n",
        "\n",
        "\n",
        "\n",
        "submission_data = []\n",
        "image_to_label = dict(zip(test_paths, predicted_labels))\n",
        "\n",
        "for img_path in test_paths:\n",
        "    img_name = os.path.basename(img_path).split('.')[0]  # Remove the file extension\n",
        "    submission_data.append((img_name, image_to_label[img_path]))\n",
        "\n",
        "print(\"Creating submission file...\")\n",
        "submission = pd.DataFrame(submission_data, columns=['Id', 'Label'])\n",
        "submission.to_csv(\"submitit.csv\", index=False)\n",
        "print(\"Submission file created successfully!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-10T10:52:09.334288Z",
          "iopub.execute_input": "2025-01-10T10:52:09.334651Z",
          "iopub.status.idle": "2025-01-10T10:52:20.021804Z",
          "shell.execute_reply.started": "2025-01-10T10:52:09.33462Z",
          "shell.execute_reply": "2025-01-10T10:52:20.020266Z"
        },
        "id": "3dSr5qTW2Csi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Data you want to append to the CSV file\n",
        "default_data = []\n",
        "\n",
        "for i in (temp_dir):\n",
        " new_i = os.path.basename(i).split('.')[0]\n",
        " default_data.append([new_i , \"Real\"])\n",
        "\n",
        "# Path to your CSV file\n",
        "csv_file_path = '/content/submito.csv'\n",
        "\n",
        "# Open the CSV file in append mode\n",
        "with open(csv_file_path, mode='a', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Append the new data as a row in the CSV\n",
        "    for j in default_data:\n",
        "     writer.writerow(j)\n"
      ],
      "metadata": {
        "id": "HFs2R0e_Nv_z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}